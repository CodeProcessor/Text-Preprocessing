{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import path as osp\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main text file was seperated into three files based on the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../data/assignment_data.txt'\n",
    "\n",
    "assignment_file = \"../data/student_corse_feedback.txt\"\n",
    "twitter_data = \"../data/twitter_data.txt\"\n",
    "research_file = \"../data/research_data.txt\"\n",
    "\n",
    "preview_length = 100\n",
    "\n",
    "output_location = '../output'\n",
    "\n",
    "class TEXT:\n",
    "    rsh = 'research'\n",
    "    twt = 'twitter'\n",
    "    asn = 'assignment'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['research'] = open(research_file, mode = 'r', encoding='utf-8').read()\n",
    "data['twitter'] = open(twitter_data, mode = 'r', encoding='utf-8').read()\n",
    "data['assignment'] = open(assignment_file, mode = 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neural network models have shown their promising opportunities for multi-task\\nlearning, which focus '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['research'][:preview_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reminds me of Liberal Immigration Fraudster Monsef avoiding deportation from Canada. #cdnpoli #LPC #'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['twitter'][:preview_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Honestly last seven lectures are good. Lectures are understandable. Lecture slides are very useful t'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['assignment'][:preview_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(text_lines, file_name):\n",
    "    full_path = osp.join(output_location, file_name)\n",
    "    dir_path = osp.dirname(full_path)\n",
    "    if not osp.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        \n",
    "    with open(full_path, 'w') as fp:\n",
    "#         print(text_lines)\n",
    "        for line in text_lines:\n",
    "#             print(line)\n",
    "            fp.write(line + ' ')\n",
    "    print(\"Written to {}\".format(full_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tockenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tockenize_text(words):\n",
    "    return word_tokenize(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Student Course Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to ../output/tockenize/assignment_output.txt\n"
     ]
    }
   ],
   "source": [
    "student_feedback_tockenize = tockenize_text(data[TEXT.asn])\n",
    "write_to_file(student_feedback_tockenize, 'tockenize/{}_output.txt'.format(TEXT.asn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Twitter Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to ../output/tockenize/twitter_output.txt\n"
     ]
    }
   ],
   "source": [
    "twitter_data_tockenize = tockenize_text(data[TEXT.twt])\n",
    "write_to_file(twitter_data_tockenize, 'tockenize/{}_output.txt'.format(TEXT.twt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Research Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to ../output/tockenize/research_output.txt\n"
     ]
    }
   ],
   "source": [
    "research_data_tockenize = tockenize_text(data[TEXT.rsh])\n",
    "write_to_file(research_data_tockenize, 'tockenize/{}_output.txt'.format(TEXT.rsh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Isolated word correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_ckecking(tockens):\n",
    "    spell = SpellChecker(distance=2)\n",
    "    mispelled = spell.unknown(tockens)\n",
    "    pairs = []\n",
    "    print(\"Mispelled count: {}\".format(len(mispelled)))\n",
    "    for i, word in enumerate(mispelled):\n",
    "        correction = spell.correction(word)\n",
    "        print('{:2} - \"{}\" is corrected as \"{}\"'.format(i, word, correction))\n",
    "        pairs.append((word, correction))\n",
    "        if i == 5:\n",
    "            break\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Student Course Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mispelled count: 14\n",
      " 0 - \"examples.lectures\" is corrected as \"examples.lectures\"\n",
      " 1 - \"''\" is corrected as \"d'\"\n",
      " 2 - \"``\" is corrected as \"of\"\n",
      " 3 - \"exercisers\" is corrected as \"exercises\"\n",
      " 4 - \"helpfull\" is corrected as \"helpful\"\n",
      " 5 - \"lectuers\" is corrected as \"lectures\"\n"
     ]
    }
   ],
   "source": [
    "word_pairs = spell_ckecking(student_feedback_tockenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Twitter Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mispelled count: 87\n",
      " 0 - \"youtube\" is corrected as \"couture\"\n",
      " 1 - \"l��\" is corrected as \"law\"\n",
      " 2 - \"5��questions\" is corrected as \"5��questions\"\n",
      " 3 - \"haryanavi\" is corrected as \"haryana\"\n",
      " 4 - \"l��immigration\" is corrected as \"l��immigration\"\n",
      " 5 - \"//t.co/nefw30mraa\" is corrected as \"//t.co/nefw30mraa\"\n"
     ]
    }
   ],
   "source": [
    "word_pairs = spell_ckecking(twitter_data_tockenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Research Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mispelled count: 16\n",
      " 0 - \"task-invariant\" is corrected as \"task-invariant\"\n",
      " 1 - \"neural-based\" is corrected as \"neural-based\"\n",
      " 2 - \"collobert\" is corrected as \"colbert\"\n",
      " 3 - \"luong\" is corrected as \"long\"\n",
      " 4 - \"1-\" is corrected as \"i-\"\n",
      " 5 - \"shared-private\" is corrected as \"shared-private\"\n"
     ]
    }
   ],
   "source": [
    "word_pairs = spell_ckecking(research_data_tockenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Context Sensitive word correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Student Course Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Twitter Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Research Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "show_stemm = 10\n",
    "def stem_tockens(tockenized_text):\n",
    "    stemmed_text = []\n",
    "    for word in tockenized_text:\n",
    "        stemmed_text.append(stemmer.stem(word))\n",
    "    print(tockenized_text[0:show_stemm])\n",
    "    print(stemmed_text[0:show_stemm])\n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Student Course Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Honestly', 'last', 'seven', 'lectures', 'are', 'good', '.', 'Lectures', 'are', 'understandable']\n",
      "['honestli', 'last', 'seven', 'lectur', 'are', 'good', '.', 'lectur', 'are', 'understand']\n",
      "Written to ../output/stemmer/assignment_output.txt\n"
     ]
    }
   ],
   "source": [
    "result = stem_tockens(student_feedback_tockenize)\n",
    "write_to_file(result, 'stemmer/{}_output.txt'.format(TEXT.asn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Twitter Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Fraudster', 'Monsef', 'avoiding', 'deportation', 'from']\n",
      "['remind', 'me', 'of', 'liber', 'immigr', 'fraudster', 'monsef', 'avoid', 'deport', 'from']\n",
      "Written to ../output/stemmer/twitter_output.txt\n"
     ]
    }
   ],
   "source": [
    "result = stem_tockens(twitter_data_tockenize)\n",
    "write_to_file(result, 'stemmer/{}_output.txt'.format(TEXT.twt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Research Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi-task']\n",
      "['neural', 'network', 'model', 'have', 'shown', 'their', 'promis', 'opportun', 'for', 'multi-task']\n",
      "Written to ../output/stemmer/research_output.txt\n"
     ]
    }
   ],
   "source": [
    "result = stem_tockens(research_data_tockenize)\n",
    "write_to_file(result, 'stemmer/{}_output.txt'.format(TEXT.rsh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "show_lemm = 10\n",
    "def lemmatize_tockens(tockenized_text):\n",
    "    lemmatized_text = []\n",
    "    for word in tockenized_text:\n",
    "        lemmatized_text.append(lemmatizer.lemmatize(word))\n",
    "    print(tockenized_text[0:show_lemm])    \n",
    "    print(lemmatized_text[0:show_lemm])\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Student Course Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Honestly', 'last', 'seven', 'lectures', 'are', 'good', '.', 'Lectures', 'are', 'understandable']\n",
      "['Honestly', 'last', 'seven', 'lecture', 'are', 'good', '.', 'Lectures', 'are', 'understandable']\n",
      "Written to ../output/lemmatize/assignment_output.txt\n"
     ]
    }
   ],
   "source": [
    "result = lemmatize_tockens(student_feedback_tockenize)\n",
    "write_to_file(result, 'lemmatize/{}_output.txt'.format(TEXT.asn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Twitter Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Fraudster', 'Monsef', 'avoiding', 'deportation', 'from']\n",
      "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Fraudster', 'Monsef', 'avoiding', 'deportation', 'from']\n",
      "Written to ../output/lemmatize/twitter_output.txt\n"
     ]
    }
   ],
   "source": [
    "result = lemmatize_tockens(twitter_data_tockenize)\n",
    "write_to_file(result, 'lemmatize/{}_output.txt'.format(TEXT.twt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Research Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi-task']\n",
      "['Neural', 'network', 'model', 'have', 'shown', 'their', 'promising', 'opportunity', 'for', 'multi-task']\n",
      "Written to ../output/lemmatize/research_output.txt\n"
     ]
    }
   ],
   "source": [
    "result = lemmatize_tockens(research_data_tockenize)\n",
    "write_to_file(result, 'lemmatize/{}_output.txt'.format(TEXT.rsh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('py376': virtualenv)",
   "language": "python",
   "name": "python37664bitpy376virtualenvc1e1d512084149469623763e9dd746d6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
