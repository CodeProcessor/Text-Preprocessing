{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import path as osp\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main text file was seperated into three files based on the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../data/assignment_data.txt'\n",
    "\n",
    "assignment_file = \"../data/student_corse_feedback.txt\"\n",
    "twitter_data = \"../data/twitter_data.txt\"\n",
    "research_file = \"../data/research_data.txt\"\n",
    "\n",
    "preview_length = 100\n",
    "\n",
    "output_location = '../output'\n",
    "\n",
    "class TEXT:\n",
    "    rsh = 'research'\n",
    "    twt = 'twitter'\n",
    "    asn = 'assignment'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['research'] = open(research_file, mode = 'r', encoding='utf-8').read()\n",
    "data['twitter'] = open(twitter_data, mode = 'r', encoding='utf-8').read()\n",
    "data['assignment'] = open(assignment_file, mode = 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neural network models have shown their promising opportunities for multi-task\\nlearning, which focus '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['research'][:preview_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reminds me of Liberal Immigration Fraudster Monsef avoiding deportation from Canada. #cdnpoli #LPC #'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['twitter'][:preview_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Honestly last seven lectures are good. Lectures are understandable. Lecture slides are very useful t'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['assignment'][:preview_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(text_lines, file_name):\n",
    "    full_path = osp.join(output_location, file_name)\n",
    "    dir_path = osp.dirname(full_path)\n",
    "    if not osp.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        \n",
    "    with open(full_path, 'w') as fp:\n",
    "#         print(text_lines)\n",
    "        for line in text_lines:\n",
    "#             print(line)\n",
    "            fp.write(line + ' ')\n",
    "    print(\"Written to {}\".format(full_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tockenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tockenize_text(words):\n",
    "    return word_tokenize(words)\n",
    "\n",
    "preview = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Student Course Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Honestly', 'last', 'seven', 'lectures', 'are', 'good', '.', 'Lectures', 'are', 'understandable', '.', 'Lecture', 'slides', 'are', 'very', 'useful', 'to', 'self-study', 'also', '.', 'The', 'given', 'opportunity', 'to', 'ask', 'questions', 'from', 'the', 'lecturer', 'is', 'appreciative', '.', '``', 'Good', ':', ')', '<', 'br', '/', '>', 'please', 'do', 'recap', 'at', 'class', 'starting', 'it', '&', '#', '039', ';', 's', 'better', 'for', 'us', '.', '<', 'br', '/', '>', 'sometimes', 'teaching', 'speed', 'is', 'very', 'high', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Thanks', '!', ':', ')', '<', 'br', '/', '>', '``', 'The', 'lectures', 'are', 'good', '..', 'but', 'a', 'bit', 'speed.A', 'in', 'class', 'working', 'activity', 'is', 'a', 'must']\n",
      "Written to ../output/tockenize/assignment_output.txt\n"
     ]
    }
   ],
   "source": [
    "student_feedback_tockenize = tockenize_text(data[TEXT.asn])\n",
    "print(student_feedback_tockenize[0:preview])\n",
    "write_to_file(student_feedback_tockenize, 'tockenize/{}_output.txt'.format(TEXT.asn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Twitter Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Fraudster', 'Monsef', 'avoiding', 'deportation', 'from', 'Canada', '.', '#', 'cdnpoli', '#', 'LPC', '#', 'CPCLDR��_', 'https', ':', '//t.co/ZOZOSe1CqQ', '#', 'immigration', '#', 'integration', '#', 'canada', 'https', ':', '//t.co/M5cKGyvV8F', 'We', 'want', 'controlled', 'immigration', 'that', 'contributes', 'positively', 'to', 'the', 'UK', 'economy', '.', 'Same', 'as', 'Australia', '&', 'amp', ';', 'Canada', '.', 'https', ':', '//t.co/99mYliuOes', 'Is', 'the', 'new', 'Manitoba', 'immigration', 'fee', 'a', 'head', 'tax', '?', 'https', ':', '//t.co/LsG7C3vLe9', 'Canada', 'immigration', 'profit', 'influence', 'modernistic', 'delhi', 'yet', 'abhinav', ':', 'XKofy', 'https', ':', '//t.co/becgusY2i6', 'Canada', 'Immigration', 'Minister', 'to', '���Substantially', 'Increase', 'Immigration', 'Numbers', 'https', ':', '//t.co/nEFw30MRaa', 'https', ':', '//t.co/cyI867PZRV', 'M��me', 'les', '#', 'USA=pays', \"d'immigration\", 'par', 'excellence']\n",
      "Written to ../output/tockenize/twitter_output.txt\n"
     ]
    }
   ],
   "source": [
    "twitter_data_tockenize = tockenize_text(data[TEXT.twt])\n",
    "print(twitter_data_tockenize[0:preview])\n",
    "write_to_file(twitter_data_tockenize, 'tockenize/{}_output.txt'.format(TEXT.twt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Research Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi-task', 'learning', ',', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', 'task-invariant', 'features', '.', 'However', ',', 'in', 'most', 'existing', 'approaches', ',', 'the', 'extracted', 'shared', 'features', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'task-specific', 'features', 'or', 'the', 'noise', 'brought', 'by', 'other', 'tasks', '.', 'In', 'this', 'paper', ',', 'we', 'propose', 'an', 'adversarial', 'multi-task', 'learning', 'framework', ',', 'alleviating', 'the', 'shared', 'and', 'private', 'latent', 'feature', 'spaces', 'from', 'interfering', 'with', 'each', 'other', '.', 'We', 'conduct', 'extensive', 'experiments', 'on', '16', 'different', 'text', 'classification', 'tasks', ',', 'which', 'demonstrates', 'the', 'benefits', 'of', 'our', 'approach', '.', 'Besides']\n",
      "Written to ../output/tockenize/research_output.txt\n"
     ]
    }
   ],
   "source": [
    "research_data_tockenize = tockenize_text(data[TEXT.rsh])\n",
    "print(research_data_tockenize[0:preview])\n",
    "write_to_file(research_data_tockenize, 'tockenize/{}_output.txt'.format(TEXT.rsh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Isolated word correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_ckecking(tockens):\n",
    "    spell = SpellChecker(distance=2)\n",
    "    mispelled = spell.unknown(tockens)\n",
    "    pairs = []\n",
    "    actually_corrected = 0\n",
    "    print(\"Mispelled count: {}\".format(len(mispelled)))\n",
    "    for i, word in enumerate(mispelled):\n",
    "        correction = spell.correction(word)\n",
    "        if correction != word:\n",
    "            actually_corrected += 1\n",
    "            if actually_corrected <= 10:\n",
    "                print('{:2} - \"{}\" is corrected as \"{}\"'.format(actually_corrected, word, correction))\n",
    "        pairs.append((word, correction))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Student Course Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mispelled count: 14\n",
      " 1 - \"speed.a\" is corrected as \"speed\"\n",
      " 2 - \"one.so\" is corrected as \"ones\"\n",
      " 3 - \"lectuers\" is corrected as \"lectures\"\n",
      " 4 - \"..\" is corrected as \"p.\"\n",
      " 5 - \"''\" is corrected as \"d'\"\n",
      " 6 - \"helpfull\" is corrected as \"helpful\"\n",
      " 7 - \"madame.\" is corrected as \"madame\"\n",
      " 8 - \"undersatand\" is corrected as \"understand\"\n",
      " 9 - \"weren\" is corrected as \"were\"\n",
      "10 - \"class.it\" is corrected as \"classic\"\n"
     ]
    }
   ],
   "source": [
    "word_pairs = spell_ckecking(student_feedback_tockenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Twitter Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mispelled count: 87\n",
      " 1 - \"'ve\" is corrected as \"eve\"\n",
      " 2 - \"https\" is corrected as \"steps\"\n",
      " 3 - \"globalist\" is corrected as \"loyalist\"\n",
      " 4 - \"'s\" is corrected as \"is\"\n",
      " 5 - \"l'immigration\" is corrected as \"immigration\"\n",
      " 6 - \"m��me\" is corrected as \"mime\"\n",
      " 7 - \"lpc\" is corrected as \"pc\"\n",
      " 8 - \"acc��s\" is corrected as \"access\"\n",
      " 9 - \"willl\" is corrected as \"will\"\n",
      "10 - \"��\" is corrected as \"of\"\n"
     ]
    }
   ],
   "source": [
    "word_pairs = spell_ckecking(twitter_data_tockenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Research Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mispelled count: 16\n",
      " 1 - \"sharable\" is corrected as \"shareable\"\n",
      " 2 - \"1-\" is corrected as \"i-\"\n",
      " 3 - \"multi-task\" is corrected as \"multi-track\"\n",
      " 4 - \"luong\" is corrected as \"long\"\n",
      " 5 - \"�infantile�\" is corrected as \"infantile\"\n",
      " 6 - \"andweston\" is corrected as \"anderton\"\n",
      " 7 - \"collobert\" is corrected as \"colbert\"\n",
      " 8 - \"herently\" is corrected as \"recently\"\n"
     ]
    }
   ],
   "source": [
    "word_pairs = spell_ckecking(research_data_tockenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Context Sensitive word correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "word_length = 2\n",
    "prefix_length = 7\n",
    "sym_spell = SymSpell(word_length, prefix_length)\n",
    "print(\"Corpus file not found\") if not sym_spell.create_dictionary(\"../data/big.txt\") else print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview = 10\n",
    "def correct_tocknized_text(words):\n",
    "    corr_count = 0\n",
    "    corrected_words = []\n",
    "    for i, word in enumerate(words[:-word_length+1]):\n",
    "        word_set = [words[i+j] for j in range(word_length)]\n",
    "        _input = ' '.join(word_set)\n",
    "        result = sym_spell.word_segmentation(_input)\n",
    "        correction = result.corrected_string\n",
    "        if correction.lower() != _input.lower() and preview > corr_count:\n",
    "            corr_count += 1\n",
    "            print('\"{}\" is corrected as \"{}\"'.format(_input, correction))\n",
    "        corrected_words.append(correction.split(' ')[0])\n",
    "    corrected_words.append(correction.split(' ')[1])\n",
    "    return corrected_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Student Course Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"good .\" is corrected as \"good a\"\n",
      "\". Lectures\" is corrected as \"a Lectures\"\n",
      "\"understandable .\" is corrected as \"understandable a\"\n",
      "\". Lecture\" is corrected as \"a Lecture\"\n",
      "\"Lecture slides\" is corrected as \"Lecture sides\"\n",
      "\"slides are\" is corrected as \"sides are\"\n",
      "\"to self-study\" is corrected as \"to self study\"\n",
      "\"self-study also\" is corrected as \"self study also\"\n",
      "\"also .\" is corrected as \"also a\"\n",
      "\". The\" is corrected as \"a The\"\n",
      "=========== Corrected ============\n",
      "['Honestly', 'last', 'seven', 'lectures', 'are', 'good', 'a', 'Lectures', 'are', 'understandable', 'a', 'Lecture', 'sides', 'are', 'very', 'useful', 'to', 'self', 'also', 'a', 'The', 'given', 'opportunity', 'to', 'ask', 'questions', 'from', 'the', 'lecturer', 'is', 'appreciative', 'a', 'of', 'Good', 'a', 'a', 'a', 'br', 'a', 'a', 'please', 'do', 'reap', 'at', 'class', 'starting', 'it', 'a', 'a', '39', 'a', 's', 'better', 'for', 'us', 'a', 'a', 'br', 'a', 'a', 'sometimes', 'teaching', 'speed', 'is', 'very', 'high', 'a', 'a', 'br', 'a', 'a', 'a', 'br', 'a', 'a', 'Thanks', 'a', 'a', 'a', 'a', 'br', 'a', 'a', 'of', 'The', 'lectures', 'are', 'good', 'of', 'but', 'a', 'bit', 'speed', 'in', 'class', 'working', 'activity', 'is', 'a', 'must']\n"
     ]
    }
   ],
   "source": [
    "corrected = correct_tocknized_text(student_feedback_tockenize)\n",
    "print(\"=========== Corrected ============\\n{}\".format(corrected[0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Twitter Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Immigration Fraudster\" is corrected as \"Immigration Frauds her\"\n",
      "\"Fraudster Monsef\" is corrected as \"Frauds her Money\"\n",
      "\"Monsef avoiding\" is corrected as \"Money avoiding\"\n",
      "\"avoiding deportation\" is corrected as \"avoiding importation\"\n",
      "\"deportation from\" is corrected as \"importation from\"\n",
      "\"Canada .\" is corrected as \"Canada a\"\n",
      "\". #\" is corrected as \"a a\"\n",
      "\"# cdnpoli\" is corrected as \"a campo li\"\n",
      "\"cdnpoli #\" is corrected as \"campo li a\"\n",
      "\"# LPC\" is corrected as \"alps\"\n",
      "=========== Corrected ============\n",
      "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Frauds', 'Money', 'avoiding', 'importation', 'from', 'Canada', 'a', 'a', 'campo', 'alps', 'Lock', 'a', 'Could', 'http', 'a', 'it', 'a', 'immigration', 'a', 'integration', 'a', 'canada', 'http', 'a', 'it', 'We', 'want', 'controlled', 'immigration', 'that', 'contributes', 'positively', 'to', 'the', 'Up', 'economy', 'a', 'Same', 'as', 'Australia', 'a', 'amp', 'a', 'Canada', 'a', 'http', 'a', 'it', 'Is', 'the', 'new', 'Monitor', 'immigration', 'fee', 'a', 'head', 'tax', 'a', 'http', 'a', 'it', 'Canada', 'immigration', 'profit', 'influence', 'modern', 'delhi', 'yet', 'china', 'a', 'Of', 'http', 'a', 'it', 'Canada', 'Immigration', 'Minister', 'to', 'of', 'Increase', 'Immigration', 'Numbers', 'http', 'a', 'it', 'http', 'a', 'of', 'Meme', 'les', 'susan', 'Us', 'dim', 'par', 'excellence']\n"
     ]
    }
   ],
   "source": [
    "corrected = correct_tocknized_text(twitter_data_tockenize)\n",
    "print(\"=========== Corrected ============\\n{}\".format(corrected[0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Research Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Neural network\" is corrected as \"Neutral network\"\n",
      "\"for multi-task\" is corrected as \"for multi ask\"\n",
      "\"multi-task learning\" is corrected as \"multi ask learning\"\n",
      "\"learning ,\" is corrected as \"learning a\"\n",
      "\", which\" is corrected as \"a which\"\n",
      "\"and task-invariant\" is corrected as \"and task in variant\"\n",
      "\"task-invariant features\" is corrected as \"task in variant features\"\n",
      "\"features .\" is corrected as \"features a\"\n",
      "\". However\" is corrected as \"a However\"\n",
      "\"However ,\" is corrected as \"However a\"\n",
      "=========== Corrected ============\n",
      "['Neutral', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi', 'learning', 'a', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', 'task', 'features', 'a', 'However', 'a', 'in', 'most', 'existing', 'approaches', 'a', 'the', 'extracted', 'shared', 'features', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'task', 'features', 'or', 'the', 'noise', 'brought', 'by', 'other', 'tasks', 'a', 'In', 'this', 'paper', 'a', 'we', 'propose', 'an', 'adversary', 'multi', 'learning', 'framework', 'a', 'alleviating', 'the', 'shared', 'and', 'private', 'latent', 'feature', 'spaces', 'from', 'interfering', 'with', 'each', 'other', 'a', 'We', 'conduct', 'extensive', 'experiments', 'on', '16', 'different', 'text', 'classification', 'tasks', 'a', 'which', 'demonstrates', 'the', 'benefits', 'of', 'our', 'approach', 'a', 'Besides']\n"
     ]
    }
   ],
   "source": [
    "corrected = correct_tocknized_text(research_data_tockenize)\n",
    "print(\"=========== Corrected ============\\n{}\".format(corrected[0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "show_stemm = 50\n",
    "def stem_tockens(tockenized_text):\n",
    "    stemmed_text = []\n",
    "    for word in tockenized_text:\n",
    "        stemmed_text.append(stemmer.stem(word))\n",
    "    print(\"========== original ===========\")\n",
    "    print(tockenized_text[0:show_stemm])\n",
    "    print(\"========== Stemmed ===========\")\n",
    "    print(stemmed_text[0:show_stemm])\n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Student Course Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== original ===========\n",
      "['Honestly', 'last', 'seven', 'lectures', 'are', 'good', '.', 'Lectures', 'are', 'understandable', '.', 'Lecture', 'slides', 'are', 'very', 'useful', 'to', 'self-study', 'also', '.', 'The', 'given', 'opportunity', 'to', 'ask', 'questions', 'from', 'the', 'lecturer', 'is', 'appreciative', '.', '``', 'Good', ':', ')', '<', 'br', '/', '>', 'please', 'do', 'recap', 'at', 'class', 'starting', 'it', '&', '#', '039']\n",
      "========== Stemmed ===========\n",
      "['honestli', 'last', 'seven', 'lectur', 'are', 'good', '.', 'lectur', 'are', 'understand', '.', 'lectur', 'slide', 'are', 'veri', 'use', 'to', 'self-studi', 'also', '.', 'the', 'given', 'opportun', 'to', 'ask', 'question', 'from', 'the', 'lectur', 'is', 'appreci', '.', '``', 'good', ':', ')', '<', 'br', '/', '>', 'pleas', 'do', 'recap', 'at', 'class', 'start', 'it', '&', '#', '039']\n",
      "\n",
      "Written to ../output/stemmer/assignment_output.txt\n"
     ]
    }
   ],
   "source": [
    "result = stem_tockens(student_feedback_tockenize)\n",
    "print(\"\")\n",
    "write_to_file(result, 'stemmer/{}_output.txt'.format(TEXT.asn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Twitter Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== original ===========\n",
      "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Fraudster', 'Monsef', 'avoiding', 'deportation', 'from', 'Canada', '.', '#', 'cdnpoli', '#', 'LPC', '#', 'CPCLDR��_', 'https', ':', '//t.co/ZOZOSe1CqQ', '#', 'immigration', '#', 'integration', '#', 'canada', 'https', ':', '//t.co/M5cKGyvV8F', 'We', 'want', 'controlled', 'immigration', 'that', 'contributes', 'positively', 'to', 'the', 'UK', 'economy', '.', 'Same', 'as', 'Australia', '&', 'amp', ';', 'Canada', '.']\n",
      "========== Stemmed ===========\n",
      "['remind', 'me', 'of', 'liber', 'immigr', 'fraudster', 'monsef', 'avoid', 'deport', 'from', 'canada', '.', '#', 'cdnpoli', '#', 'lpc', '#', 'cpcldr��_', 'http', ':', '//t.co/zozose1cqq', '#', 'immigr', '#', 'integr', '#', 'canada', 'http', ':', '//t.co/m5ckgyvv8f', 'We', 'want', 'control', 'immigr', 'that', 'contribut', 'posit', 'to', 'the', 'UK', 'economi', '.', 'same', 'as', 'australia', '&', 'amp', ';', 'canada', '.']\n",
      "Written to ../output/stemmer/twitter_output.txt\n"
     ]
    }
   ],
   "source": [
    "result = stem_tockens(twitter_data_tockenize)\n",
    "write_to_file(result, 'stemmer/{}_output.txt'.format(TEXT.twt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Research Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== original ===========\n",
      "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi-task', 'learning', ',', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', 'task-invariant', 'features', '.', 'However', ',', 'in', 'most', 'existing', 'approaches', ',', 'the', 'extracted', 'shared', 'features', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'task-specific', 'features', 'or', 'the', 'noise', 'brought']\n",
      "========== Stemmed ===========\n",
      "['neural', 'network', 'model', 'have', 'shown', 'their', 'promis', 'opportun', 'for', 'multi-task', 'learn', ',', 'which', 'focu', 'on', 'learn', 'the', 'share', 'layer', 'to', 'extract', 'the', 'common', 'and', 'task-invari', 'featur', '.', 'howev', ',', 'in', 'most', 'exist', 'approach', ',', 'the', 'extract', 'share', 'featur', 'are', 'prone', 'to', 'be', 'contamin', 'by', 'task-specif', 'featur', 'or', 'the', 'nois', 'brought']\n",
      "Written to ../output/stemmer/research_output.txt\n"
     ]
    }
   ],
   "source": [
    "result = stem_tockens(research_data_tockenize)\n",
    "write_to_file(result, 'stemmer/{}_output.txt'.format(TEXT.rsh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "show_lemm = 50\n",
    "def lemmatize_tockens(tockenized_text):\n",
    "    lemmatized_text = []\n",
    "    for word in tockenized_text:\n",
    "        lemmatized_text.append(lemmatizer.lemmatize(word))\n",
    "    print(\"========== original ===========\")\n",
    "    print(tockenized_text[0:show_lemm])    \n",
    "    print(\"========== Lemmatized ===========\")\n",
    "    print(lemmatized_text[0:show_lemm])\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Student Course Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== original ===========\n",
      "['Honestly', 'last', 'seven', 'lectures', 'are', 'good', '.', 'Lectures', 'are', 'understandable', '.', 'Lecture', 'slides', 'are', 'very', 'useful', 'to', 'self-study', 'also', '.', 'The', 'given', 'opportunity', 'to', 'ask', 'questions', 'from', 'the', 'lecturer', 'is', 'appreciative', '.', '``', 'Good', ':', ')', '<', 'br', '/', '>', 'please', 'do', 'recap', 'at', 'class', 'starting', 'it', '&', '#', '039']\n",
      "========== Lemmatized ===========\n",
      "['Honestly', 'last', 'seven', 'lecture', 'are', 'good', '.', 'Lectures', 'are', 'understandable', '.', 'Lecture', 'slide', 'are', 'very', 'useful', 'to', 'self-study', 'also', '.', 'The', 'given', 'opportunity', 'to', 'ask', 'question', 'from', 'the', 'lecturer', 'is', 'appreciative', '.', '``', 'Good', ':', ')', '<', 'br', '/', '>', 'please', 'do', 'recap', 'at', 'class', 'starting', 'it', '&', '#', '039']\n",
      "\n",
      "Written to ../output/lemmatize/assignment_output.txt\n"
     ]
    }
   ],
   "source": [
    "result = lemmatize_tockens(student_feedback_tockenize)\n",
    "print(\"\")\n",
    "write_to_file(result, 'lemmatize/{}_output.txt'.format(TEXT.asn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Twitter Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== original ===========\n",
      "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Fraudster', 'Monsef', 'avoiding', 'deportation', 'from', 'Canada', '.', '#', 'cdnpoli', '#', 'LPC', '#', 'CPCLDR��_', 'https', ':', '//t.co/ZOZOSe1CqQ', '#', 'immigration', '#', 'integration', '#', 'canada', 'https', ':', '//t.co/M5cKGyvV8F', 'We', 'want', 'controlled', 'immigration', 'that', 'contributes', 'positively', 'to', 'the', 'UK', 'economy', '.', 'Same', 'as', 'Australia', '&', 'amp', ';', 'Canada', '.']\n",
      "========== Lemmatized ===========\n",
      "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Fraudster', 'Monsef', 'avoiding', 'deportation', 'from', 'Canada', '.', '#', 'cdnpoli', '#', 'LPC', '#', 'CPCLDR��_', 'http', ':', '//t.co/ZOZOSe1CqQ', '#', 'immigration', '#', 'integration', '#', 'canada', 'http', ':', '//t.co/M5cKGyvV8F', 'We', 'want', 'controlled', 'immigration', 'that', 'contributes', 'positively', 'to', 'the', 'UK', 'economy', '.', 'Same', 'a', 'Australia', '&', 'amp', ';', 'Canada', '.']\n",
      "\n",
      "Written to ../output/lemmatize/twitter_output.txt\n"
     ]
    }
   ],
   "source": [
    "result = lemmatize_tockens(twitter_data_tockenize)\n",
    "print(\"\")\n",
    "write_to_file(result, 'lemmatize/{}_output.txt'.format(TEXT.twt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Research Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== original ===========\n",
      "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi-task', 'learning', ',', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', 'task-invariant', 'features', '.', 'However', ',', 'in', 'most', 'existing', 'approaches', ',', 'the', 'extracted', 'shared', 'features', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'task-specific', 'features', 'or', 'the', 'noise', 'brought']\n",
      "========== Lemmatized ===========\n",
      "['Neural', 'network', 'model', 'have', 'shown', 'their', 'promising', 'opportunity', 'for', 'multi-task', 'learning', ',', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layer', 'to', 'extract', 'the', 'common', 'and', 'task-invariant', 'feature', '.', 'However', ',', 'in', 'most', 'existing', 'approach', ',', 'the', 'extracted', 'shared', 'feature', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'task-specific', 'feature', 'or', 'the', 'noise', 'brought']\n",
      "\n",
      "Written to ../output/lemmatize/research_output.txt\n"
     ]
    }
   ],
   "source": [
    "result = lemmatize_tockens(research_data_tockenize)\n",
    "print(\"\")\n",
    "write_to_file(result, 'lemmatize/{}_output.txt'.format(TEXT.rsh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('py376': virtualenv)",
   "language": "python",
   "name": "python37664bitpy376virtualenvc1e1d512084149469623763e9dd746d6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
